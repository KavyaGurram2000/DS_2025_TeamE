{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnnualValue_AllHomes</th>\n",
       "      <th>Previous_AnnualValue_AllHomes</th>\n",
       "      <th>Annual_Increase_AllHomes</th>\n",
       "      <th>Personal_Income_Growth</th>\n",
       "      <th>Population_Growth</th>\n",
       "      <th>Per_Capita_Income_Growth</th>\n",
       "      <th>County_Integer</th>\n",
       "      <th>State_Integer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_Recorded</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>165852.00</td>\n",
       "      <td>150023.33</td>\n",
       "      <td>11.00</td>\n",
       "      <td>9.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01</th>\n",
       "      <td>182380.00</td>\n",
       "      <td>165852.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>192865.33</td>\n",
       "      <td>182380.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>9.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01</th>\n",
       "      <td>194390.83</td>\n",
       "      <td>192865.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>202480.83</td>\n",
       "      <td>194390.83</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               AnnualValue_AllHomes  Previous_AnnualValue_AllHomes  \\\n",
       "Year_Recorded                                                        \n",
       "2020-01-01                165852.00                      150023.33   \n",
       "2021-01-01                182380.00                      165852.00   \n",
       "2022-01-01                192865.33                      182380.00   \n",
       "2023-01-01                194390.83                      192865.33   \n",
       "2024-01-01                202480.83                      194390.83   \n",
       "\n",
       "               Annual_Increase_AllHomes  Personal_Income_Growth  \\\n",
       "Year_Recorded                                                     \n",
       "2020-01-01                        11.00                    9.60   \n",
       "2021-01-01                        10.00                    9.60   \n",
       "2022-01-01                         6.00                    9.60   \n",
       "2023-01-01                         1.00                    2.20   \n",
       "2024-01-01                         4.00                    7.70   \n",
       "\n",
       "               Population_Growth  Per_Capita_Income_Growth  County_Integer  \\\n",
       "Year_Recorded                                                                \n",
       "2020-01-01                  0.50                      9.10               1   \n",
       "2021-01-01                  0.50                      9.10               1   \n",
       "2022-01-01                  0.50                      9.10               1   \n",
       "2023-01-01                  0.90                      1.30               1   \n",
       "2024-01-01                  1.00                      6.60               1   \n",
       "\n",
       "               State_Integer  \n",
       "Year_Recorded                 \n",
       "2020-01-01                 1  \n",
       "2021-01-01                 1  \n",
       "2022-01-01                 1  \n",
       "2023-01-01                 1  \n",
       "2024-01-01                 1  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install scikeras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score,TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import pmdarima as pm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load Data from input file\n",
    "file_path = \"zillow_combined_zhvi_sb_updated.csv\" ##Sourec file generated after cleaning data and processing\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# set the correct date format\n",
    "data['Year_Recorded'] = pd.to_datetime(data['Year_Recorded'].astype(str) + \"-01-01\")\n",
    "data.set_index('Year_Recorded', inplace=True)\n",
    "\n",
    "# Select relevant features from file\n",
    "features = ['AnnualValue_AllHomes', 'Previous_AnnualValue_AllHomes', 'Annual_Increase_AllHomes',\n",
    "            'Personal_Income_Growth', 'Population_Growth', 'Per_Capita_Income_Growth','County_Integer','State_Integer']\n",
    "data = data[features].dropna()\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test Split based on Year_Recorded  \n",
    "\n",
    "target_column = 'AnnualValue_AllHomes'\n",
    "\n",
    "# Reset index to access Year_Recorded as a column\n",
    "data = data.reset_index()\n",
    "\n",
    "# Define features (excluding the target column and Year_Recorded)\n",
    "features = [col for col in data.columns if col not in [target_column, 'Year_Recorded']]\n",
    "\n",
    "# Train/Validation/Test split\n",
    "train_df = data[(data['Year_Recorded'].dt.year < 2023) & (data[target_column] != 0)][features + [target_column, 'Year_Recorded']]\n",
    "val_df = data[(data['Year_Recorded'].dt.year == 2023) & (data[target_column] != 0)][features + [target_column, 'Year_Recorded']]\n",
    "test_df = data[(data['Year_Recorded'].dt.year == 2024) & (data[target_column] != 0)][features + [target_column, 'Year_Recorded']].copy()\n",
    "\n",
    "# Modify test year to 2025\n",
    "test_df['Year_Recorded'] = 2024\n",
    "\n",
    "# Define train, validation, and test sizes\n",
    "train_size = len(train_df)\n",
    "val_size = len(val_df)\n",
    "test_size = len(test_df)\n",
    "\n",
    "# Assign train, val, and test sets\n",
    "train, val, test = train_df, val_df, test_df  \n",
    "\n",
    "# Splitting into features (X) and target (y)\n",
    "X_train, X_val, X_test = train.drop(columns=[target_column]), val.drop(columns=[target_column]), test.drop(columns=[target_column])\n",
    "y_train, y_val, y_test = train[target_column], val[target_column], test[target_column]\n",
    "\n",
    "# Keep the corresponding years separately\n",
    "train_years, val_years, test_years = train[['Year_Recorded', target_column,'County_Integer','State_Integer']].reset_index(drop=True), \\\n",
    "                                     val[['Year_Recorded', target_column,'County_Integer','State_Integer']].reset_index(drop=True), \\\n",
    "                                     test[['Year_Recorded', target_column,'County_Integer','State_Integer']].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARIMA RMSE on Validation: 255036.57\n",
      "SARIMA RMSE on Validation: 128082.67\n",
      "SARIMA RMSE on Validation: 195419.43\n",
      "SARIMA RMSE on Validation: 238994.87\n",
      "SARIMA RMSE on Validation: 192261.11\n",
      "SARIMA Forecast :\n",
      "            Year  AnnualValue_AllHomes\n",
      "6483  2025-01-01             338470.76\n",
      "6484  2025-01-01             358768.94\n",
      "6485  2025-01-01             447745.75\n",
      "6486  2025-01-01             485763.64\n",
      "6487  2025-01-01             576246.84\n",
      "...          ...                   ...\n",
      "8639  2025-01-01            1099440.29\n",
      "8640  2025-01-01            1041689.17\n",
      "8641  2025-01-01            1074063.10\n",
      "8642  2025-01-01            1091890.82\n",
      "8643  2025-01-01            1050145.10\n",
      "\n",
      "[2161 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Ensure index is in datetime format\n",
    "train.index = pd.to_datetime(train.index)\n",
    "\n",
    "# Get the last training year\n",
    "last_train_year = test['Year_Recorded'].max()\n",
    "\n",
    "# Define future years for forecasting\n",
    "future_years = '2025-01-01' ##pd.date_range(start='2025-01-01', periods=10, freq='Y')\n",
    "\n",
    "# Time Series Cross Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for train_idx, val_idx in tscv.split(y_train):\n",
    "    y_train_cv, y_val_cv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Fit SARIMA model\n",
    "    sarima_model = SARIMAX(y_train_cv, order=(3,1,3), seasonal_order=(1,1,1,12))\n",
    "    sarima_fit = sarima_model.fit()\n",
    "    \n",
    "    # Forecast for validation set\n",
    "    val_forecast = sarima_fit.forecast(steps=len(y_val_cv))\n",
    "    \n",
    "    # Compute RMSE\n",
    "    print(f\"SARIMA RMSE on Validation: {mean_squared_error(y_val_cv, val_forecast, squared=False):.2f}\")\n",
    "\n",
    "# Final Model Validation\n",
    "sarima_final_model = SARIMAX(y_train, order=(3,1,3), seasonal_order=(1,1,1,12))\n",
    "sarima_final_fit = sarima_final_model.fit()\n",
    "\n",
    "# Forecast for validation, test, and future years\n",
    "sarima_val_forecast = sarima_final_fit.forecast(steps=len(val))\n",
    "sarima_test_forecast = sarima_final_fit.forecast(steps=len(test))\n",
    "sarima_forecast = sarima_final_fit.forecast(steps=len(test))\n",
    "\n",
    "# Convert forecast to DataFrame\n",
    "sarima_forecast_df = pd.DataFrame({\n",
    "    'Year': future_years,\n",
    "    'AnnualValue_AllHomes': sarima_forecast\n",
    "})\n",
    "\n",
    "# Print results\n",
    "print(\"SARIMA Forecast :\")\n",
    "print(sarima_forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:31:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:31:21 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet Validation Forecast:\n",
      "             ds                    yhat\n",
      "0    2023-01-01 27350671364774969344.00\n",
      "1    2023-01-01 27350671364774969344.00\n",
      "2    2023-01-01 27350671364774969344.00\n",
      "3    2023-01-01 27350671364774969344.00\n",
      "4    2023-01-01 27350671364774969344.00\n",
      "...         ...                     ...\n",
      "2156 2023-01-01 27350671364774969344.00\n",
      "2157 2023-01-01 27350671364774969344.00\n",
      "2158 2023-01-01 27350671364774969344.00\n",
      "2159 2023-01-01 27350671364774969344.00\n",
      "2160 2023-01-01 27350671364774969344.00\n",
      "\n",
      "[2161 rows x 2 columns]\n",
      "Prophet Forecast for 2025-01-01:\n",
      "             ds                    yhat\n",
      "0    2025-01-01 28383491943742337024.00\n",
      "1    2025-01-01 28383491943742337024.00\n",
      "2    2025-01-01 28383491943742337024.00\n",
      "3    2025-01-01 28383491943742337024.00\n",
      "4    2025-01-01 28383491943742337024.00\n",
      "...         ...                     ...\n",
      "2156 2025-01-01 28383491943742337024.00\n",
      "2157 2025-01-01 28383491943742337024.00\n",
      "2158 2025-01-01 28383491943742337024.00\n",
      "2159 2025-01-01 28383491943742337024.00\n",
      "2160 2025-01-01 28383491943742337024.00\n",
      "\n",
      "[2161 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Prophet Hyperparameter Tuning \n",
    "def train_prophet(train_data, val_data, test_data):\n",
    "    # Prepare training data\n",
    "    prophet_df = train_data[['AnnualValue_AllHomes']].reset_index()\n",
    "    prophet_df.columns = ['ds', 'y']\n",
    "    prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])  # Ensure datetime format\n",
    "    \n",
    "    # Initialize and fit Prophet model\n",
    "    prophet_model = Prophet(changepoint_prior_scale=0.05, seasonality_mode='multiplicative')\n",
    "    prophet_model.fit(prophet_df)\n",
    "\n",
    "    #  Ensure val_data['ds'] is in datetime format\n",
    "    val_data = val_data.reset_index()\n",
    "    val_data['ds'] = pd.to_datetime(val_data['Year_Recorded'])  # Assuming 'Year_Recorded' is the correct column\n",
    "    future_val = val_data[['ds']]\n",
    "\n",
    "    # Make predictions\n",
    "    prophet_val_forecast = prophet_model.predict(future_val)[['ds', 'yhat']]\n",
    "    \n",
    "    # Future forecast with a fixed date\n",
    "    forecast_rows = len(test_data)  # Ensure test_data is passed correctly\n",
    "    future_fixed_date = pd.DataFrame({'ds': ['2025-01-01'] * forecast_rows})  # Fixed date for all rows\n",
    "    prophet_forecast = prophet_model.predict(future_fixed_date)[['ds', 'yhat']]\n",
    "    \n",
    "    return prophet_val_forecast, prophet_forecast\n",
    "\n",
    "# Call the function and print results\n",
    "prophet_val_forecast, prophet_future_forecast = train_prophet(train, val, test)\n",
    "\n",
    "print(\"Prophet Validation Forecast:\")\n",
    "print(prophet_val_forecast)\n",
    "\n",
    "print(\"Prophet Forecast for 2025-01-01:\")\n",
    "print(prophet_future_forecast)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ARIMA Params: (3, 1, 0)\n",
      "Best RMSE: 132695.7772192363\n",
      "      Year  Forecasted_AnnualValue_AllHomes\n",
      "6483  2025                        258349.44\n",
      "6484  2025                        248381.90\n",
      "6485  2025                        238081.24\n",
      "6486  2025                        231694.97\n",
      "6487  2025                        236452.12\n",
      "...    ...                              ...\n",
      "8639  2025                        238464.77\n",
      "8640  2025                        238464.77\n",
      "8641  2025                        238464.77\n",
      "8642  2025                        238464.77\n",
      "8643  2025                        238464.77\n",
      "\n",
      "[2161 rows x 2 columns]\n",
      "ARIMA Test RMSE: 196960.10\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Function to tune ARIMA model using Grid Search\n",
    "def tune_arima_model(y_train, max_p=3, max_q=3):  \n",
    "    # Initialize TimeSeriesSplit for Cross-Validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Define a range for the parameters to try \n",
    "    p_range = range(0, max_p+1)\n",
    "    q_range = range(0, max_q+1)\n",
    "    d_range = [0, 1]\n",
    "    \n",
    "    # Grid search over all combinations of parameters\n",
    "    for p in p_range:\n",
    "        for q in q_range:\n",
    "            for d in d_range:\n",
    "                try:\n",
    "                    # Train ARIMA model with the current set of parameters\n",
    "                    arima_model = ARIMA(y_train, order=(p,d,q))  # ARIMA has only order parameter\n",
    "                    arima_fit = arima_model.fit()  # Removed disp=False\n",
    "                    \n",
    "                    # Validate ARIMA model using TimeSeriesSplit (for cross-validation)\n",
    "                    rmse_values = []\n",
    "                    for train_idx, val_idx in tscv.split(y_train):\n",
    "                        y_train_cv, y_val_cv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                        arima_cv_model = ARIMA(y_train_cv, order=(p,d,q))  # Same order for cross-validation\n",
    "                        arima_cv_fit = arima_cv_model.fit()  # Removed disp=False\n",
    "                        \n",
    "                        val_forecast = arima_cv_fit.forecast(steps=len(y_val_cv))\n",
    "                        rmse = np.sqrt(mean_squared_error(y_val_cv, val_forecast))\n",
    "                        rmse_values.append(rmse)\n",
    "                    \n",
    "                    # Calculate the average RMSE for the current parameter set\n",
    "                    avg_rmse = np.mean(rmse_values)\n",
    "                    \n",
    "                    # If the RMSE is the best so far, save the model and parameters\n",
    "                    if avg_rmse < best_rmse:\n",
    "                        best_rmse = avg_rmse\n",
    "                        best_params = (p, d, q)\n",
    "                        best_model = arima_fit\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting ARIMA model with params {(p, d, q)}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Best ARIMA Params: {best_params}\")\n",
    "    print(f\"Best RMSE: {best_rmse}\")\n",
    "    \n",
    "    return best_model, best_params, best_rmse\n",
    "\n",
    "# tuning ARIMA model\n",
    "best_arima_model, best_arima_params, best_arima_rmse = tune_arima_model(y_train)\n",
    "\n",
    "# Check if we found a model\n",
    "if best_arima_model is not None:\n",
    "    # Forecast using the best ARIMA model \n",
    "    arima_forecast = best_arima_model.forecast(steps=len(test))\n",
    "\n",
    "    # Create a date range starting from 2025 \n",
    "    #forecast_years = pd.date_range(start='2025', periods=0, freq='Y')\n",
    "    forecast_years ='2025'\n",
    "    \n",
    "    # Convert the forecast into a DataFrame with the years\n",
    "    arima_forecast_df = pd.DataFrame({\n",
    "        'Year': forecast_years,\n",
    "        'Forecasted_AnnualValue_AllHomes': arima_forecast\n",
    "    })\n",
    "\n",
    "    # Print the results\n",
    "    print(arima_forecast_df)\n",
    "\n",
    "    # Evaluate the model on the test data \n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, arima_forecast))\n",
    "    print(f\"ARIMA Test RMSE: {test_rmse:.2f}\")\n",
    "else:\n",
    "    print(\"No valid ARIMA model was found.\")\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([216472.20861217, 288822.06016185, 132644.79479457, 121164.35904201,\n",
       "       146758.89383426, 120978.30902594, 146223.29026357, 132993.64494258,\n",
       "       223529.79688222, 125149.60981382])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Start running from here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Drop 'Year_Recorded' as it's a datetime column\n",
    "X_train_filtered = X_train.drop(columns=['Year_Recorded'])\n",
    "X_val_filtered = X_val.drop(columns=['Year_Recorded'])\n",
    "X_test_filtered = X_test.drop(columns=['Year_Recorded'])\n",
    "\n",
    "# PCA for Dimensionality Reduction\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
    "X_val_scaled = scaler.transform(X_val_filtered)\n",
    "X_test_scaled = scaler.transform(X_test_filtered)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Hyperparameter Tuning for Random Forest\n",
    "rf_param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20]}\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_grid.fit(X_train_pca, y_train)\n",
    "\n",
    "# Train the best model\n",
    "rf_model = rf_grid.best_estimator_\n",
    "rf_predictions = rf_model.predict(X_test_pca)\n",
    "\n",
    "# Display  predictions\n",
    "rf_predictions[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202067.32370177, 357966.9188653 , 146225.63118061, 197386.532464  ,\n",
       "       230941.44054881,  98983.92225453, 172082.08228164, 166939.97367713,\n",
       "       222202.96307342, 133744.51527158])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Hyperparameter Tuning \n",
    "# Gradient Boosting: Gradient boosting builds models sequentially to correct errors in previous predictions.\n",
    "# Key Strength: Excellent for structured data and can outperform other tree-based models when tuned properly.\n",
    "# Drop 'Year_Recorded' as it's a datetime column\n",
    "X_train_filtered = X_train.drop(columns=['Year_Recorded'])\n",
    "X_val_filtered = X_val.drop(columns=['Year_Recorded'])\n",
    "X_test_filtered = X_test.drop(columns=['Year_Recorded'])\n",
    "\n",
    "# Gradient Boosting Hyperparameter Tuning\n",
    "gb_param_grid = {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]}\n",
    "gb_grid = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model with the cleaned data\n",
    "gb_grid.fit(X_train_filtered, y_train)\n",
    "\n",
    "# Train the best model\n",
    "gb_model = gb_grid.best_estimator_\n",
    "gb_predictions = gb_model.predict(X_test_filtered)\n",
    "\n",
    "# Display first 10 predictions\n",
    "gb_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([204165.53, 353007.22, 147208.89, 198120.66, 229650.58, 100467.6 ,\n",
       "       172386.12, 168954.78, 219807.1 , 136823.05], dtype=float32)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost Hyperparameter Tuning \n",
    "# XGBoost- An optimized version of gradient boosting, XGBoost improves computational efficiency and predictive accuracy.\n",
    "# Key Strength: Handles missing values well and prevents overfitting using regularization techniques.\n",
    "# Drop 'Year_Recorded' column\n",
    "X_train_filtered = X_train.drop(columns=['Year_Recorded'])\n",
    "X_test_filtered = X_test.drop(columns=['Year_Recorded'])\n",
    "\n",
    "# Define parameter grid\n",
    "xgb_param_grid = {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1]}\n",
    "\n",
    "# Perform Grid Search\n",
    "xgb_grid = GridSearchCV(XGBRegressor(random_state=42), xgb_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "xgb_grid.fit(X_train_filtered, y_train)\n",
    "\n",
    "# Get best model\n",
    "xgb_model = xgb_grid.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "xgb_predictions = xgb_model.predict(X_test_filtered)\n",
    "xgb_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([199062.13657351, 359217.31081104, 135177.77889195, 193120.30915633,\n",
       "       226561.06827671,  86068.50316705, 166240.19534692, 162684.56216609,\n",
       "       216678.38423103, 123112.57088919])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression (L2 Regularization) \n",
    "# Ridge Regression - Ridge regression applies L2 regularization to control model complexity, \n",
    "# making it useful for linear relationships.\n",
    "# Key Strength: Prevents overfitting and ensures stability in regression-based predictions.\n",
    "# Drop datetime column or convert it\n",
    "# Ensure 'Year_Recorded' is a datetime column before extracting the year\n",
    "X_train['Year_Recorded'] = pd.to_datetime(X_train['Year_Recorded'], errors='coerce')\n",
    "X_test['Year_Recorded'] = pd.to_datetime(X_test['Year_Recorded'], errors='coerce')\n",
    "\n",
    "X_train['Year_Recorded'] = X_train['Year_Recorded'].dt.year\n",
    "X_test['Year_Recorded'] = X_test['Year_Recorded'].dt.year\n",
    "\n",
    "# Define parameter grid\n",
    "ridge_param_grid = {'alpha': [0.1, 1, 10]}\n",
    "\n",
    "# Perform Grid Search\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "ridge_model = ridge_grid.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "ridge_predictions = ridge_model.predict(X_test)\n",
    "ridge_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "406/406 [==============================] - 6s 6ms/step - loss: 85198594048.0000\n",
      "Epoch 2/20\n",
      "406/406 [==============================] - 3s 6ms/step - loss: 84679581696.0000\n",
      "Epoch 3/20\n",
      "406/406 [==============================] - 3s 6ms/step - loss: 83725524992.0000\n",
      "Epoch 4/20\n",
      "406/406 [==============================] - 3s 7ms/step - loss: 82409914368.0000\n",
      "Epoch 5/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 80781131776.0000\n",
      "Epoch 6/20\n",
      "406/406 [==============================] - 4s 10ms/step - loss: 78882455552.0000\n",
      "Epoch 7/20\n",
      "406/406 [==============================] - 4s 9ms/step - loss: 76792283136.0000\n",
      "Epoch 8/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 74514833408.0000\n",
      "Epoch 9/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 72106688512.0000\n",
      "Epoch 10/20\n",
      "406/406 [==============================] - 3s 9ms/step - loss: 69539332096.0000\n",
      "Epoch 11/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 66924388352.0000\n",
      "Epoch 12/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 64294113280.0000\n",
      "Epoch 13/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 61541261312.0000\n",
      "Epoch 14/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 58875596800.0000\n",
      "Epoch 15/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 56166563840.0000\n",
      "Epoch 16/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 53460652032.0000\n",
      "Epoch 17/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 50843148288.0000\n",
      "Epoch 18/20\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 48289468416.0000\n",
      "Epoch 19/20\n",
      "406/406 [==============================] - 4s 9ms/step - loss: 45842575360.0000\n",
      "Epoch 20/20\n",
      "406/406 [==============================] - 4s 9ms/step - loss: 43570855936.0000\n",
      "136/136 [==============================] - 1s 3ms/step\n",
      "[118730.49 118730.49 118730.49 118730.49 118730.49 118730.49 118730.49\n",
      " 118730.49 118730.49 118730.49]\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model with KerasRegressor for Tuning\n",
    "# LSTM (Long Short-Term Memory) Neural Network:- LSTM is a deep learning model specifically designed for sequential data,\n",
    "# making it effective for time series forecasting.\n",
    "# Key Strength: Captures long-term dependencies and non-linear relationships in the data.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(X_train_reshaped.shape[1], 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "lstm_regressor = KerasRegressor(build_fn=build_lstm_model, epochs=20, batch_size=16, verbose=1)\n",
    "lstm_regressor.fit(X_train_reshaped, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lstm_predictions = lstm_regressor.predict(X_test_reshaped)\n",
    "print(lstm_predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model      RMSE       MAE  MAPE     R²\n",
      "0             SARIMA 281064.66 253197.74  1.65 -17.77\n",
      "1              ARIMA  79606.35  69014.93  0.45  -0.51\n",
      "2      Random Forest  44015.10  34639.36  0.18   0.54\n",
      "3  Gradient Boosting   3036.49   1898.68  0.01   1.00\n",
      "4            XGBoost   1566.90   1155.61  0.01   1.00\n",
      "5   Ridge Regression   8086.29   7119.27  0.05   0.98\n",
      "6               LSTM  97844.89  77337.87  0.35  -1.27\n",
      "\n",
      "Best Model: XGBoost with RMSE 1566.90\n"
     ]
    }
   ],
   "source": [
    "#evaluation results \n",
    "# Set float display format to avoid scientific notation\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "evaluation_data = []\n",
    "\n",
    "y_test_val = y_test[:10]\n",
    "#y_test_val = y_test\n",
    "\n",
    "def evaluate_model(name, actual, predicted):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mape = mean_absolute_percentage_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    \n",
    "    # Append results to the evaluation_data list\n",
    "    evaluation_data.append([name, rmse, mae, mape, r2])\n",
    "    \n",
    "    # Return all evaluation metrics as a tuple\n",
    "    return (rmse, mae, mape, r2)\n",
    "arima_forecast_sliced = arima_forecast_df[[\"Forecasted_AnnualValue_AllHomes\"]].head(10).values.flatten()\n",
    "\n",
    "# Evaluate models\n",
    "evaluation_results = {\n",
    "     \"SARIMA\": evaluate_model(\"SARIMA\", y_test_val, sarima_forecast.values[:10]),\n",
    "    # \"Prophet\": evaluate_model(\"Prophet\", y_test_val, prophet_future_forecast[['yhat']]),\n",
    "     \"ARIMA\": evaluate_model(\"ARIMA\", y_test_val, arima_forecast_sliced),\n",
    "    \"Random Forest\": evaluate_model(\"Random Forest\", y_test_val, rf_predictions[:10]),\n",
    "    \"Gradient Boosting\": evaluate_model(\"Gradient Boosting\", y_test_val, gb_predictions[:10]),\n",
    "    \"XGBoost\": evaluate_model(\"XGBoost\", y_test_val, xgb_predictions[:10]),\n",
    "    \"Ridge Regression\": evaluate_model(\"Ridge Regression\", y_test_val, ridge_predictions[:10]),\n",
    "    \"LSTM\": evaluate_model(\"LSTM\", y_test_val, lstm_predictions[:10])\n",
    "}\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "evaluation_df = pd.DataFrame(evaluation_data, columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R²\"])\n",
    "\n",
    "# Display results in tabular format\n",
    "print(evaluation_df)\n",
    "\n",
    "# Find and print best model based on RMSE (first element of the tuple)\n",
    "best_model = min(evaluation_results, key=lambda x: evaluation_results[x][0])  # Get model with minimum RMSE\n",
    "print(f\"\\nBest Model: {best_model} with RMSE {evaluation_results[best_model][0]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Train MAE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Train R2</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5786.20</td>\n",
       "      <td>57702.05</td>\n",
       "      <td>1791.39</td>\n",
       "      <td>5716.39</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>99.81</td>\n",
       "      <td>99.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train RMSE  Test RMSE  Train MAE  Test MAE  Train R2  Test R2  \\\n",
       "0     5786.20   57702.05    1791.39   5716.39      1.00     0.91   \n",
       "\n",
       "   Train Accuracy  Test Accuracy  \n",
       "0           99.81          99.21  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_best_model(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "   \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "   \n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "   \n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "   \n",
    "    # Calculate accuracy as the percentage of predictions within a certain tolerance (e.g., 10%)\n",
    "    tolerance = 0.1\n",
    "    train_accuracy = np.mean(np.abs((y_train - y_pred_train) / y_train) < tolerance) * 100\n",
    "    test_accuracy = np.mean(np.abs((y_test - y_pred_test) / y_test) < tolerance) * 100\n",
    "   \n",
    "   \n",
    "    results_df = pd.DataFrame({\n",
    "        'Train RMSE': [train_rmse],\n",
    "        'Test RMSE': [test_rmse],\n",
    "        'Train MAE': [train_mae],\n",
    "        'Test MAE': [test_mae],\n",
    "        'Train R2': [train_r2],\n",
    "        'Test R2': [test_r2],\n",
    "        'Train Accuracy': [train_accuracy],\n",
    "        'Test Accuracy': [test_accuracy]\n",
    "    })\n",
    "   \n",
    "    return results_df\n",
    " \n",
    "# Ensure 'Year_Recorded' is not in the test data if it was not used in training\n",
    "X_train = X_train.drop(columns=['Year_Recorded'], errors='ignore')\n",
    "X_test = X_test.drop(columns=['Year_Recorded'], errors='ignore')\n",
    "\n",
    "# Now make predictions\n",
    "results_allhomes = evaluate_best_model(\n",
    "        xgb_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "results_allhomes\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Results:\n",
      "  Year_Recorded  County_Integer  State_Integer  AnnualValue_AllHomes  \\\n",
      "0    2020-01-01               1              1             165852.00   \n",
      "1    2021-01-01               1              1             182380.00   \n",
      "2    2022-01-01               1              1             192865.33   \n",
      "3    2020-01-01               2              1             257352.71   \n",
      "4    2021-01-01               2              1             306665.82   \n",
      "\n",
      "   Predicted  \n",
      "0  167986.58  \n",
      "1  181488.48  \n",
      "2  193334.12  \n",
      "3  259227.98  \n",
      "4  306047.28  \n",
      "\n",
      "Validation Results:\n",
      "  Year_Recorded  County_Integer  State_Integer  AnnualValue_AllHomes  \\\n",
      "0    2023-01-01               1              1             194390.83   \n",
      "1    2023-01-01               2              1             345427.18   \n",
      "2    2023-01-01               3              1             142336.67   \n",
      "3    2023-01-01               4              1             192880.50   \n",
      "4    2023-01-01               5              1             222670.75   \n",
      "\n",
      "   Predicted  \n",
      "0  193799.28  \n",
      "1  348790.91  \n",
      "2  145139.23  \n",
      "3  195240.38  \n",
      "4  226399.11  \n",
      "\n",
      "Test Results:\n",
      "   Year_Recorded  County_Integer  State_Integer  AnnualValue_AllHomes  \\\n",
      "0           2024               1              1             202480.83   \n",
      "1           2024               2              1             349517.88   \n",
      "2           2024               3              1             145776.00   \n",
      "3           2024               4              1             198461.25   \n",
      "4           2024               5              1             229328.75   \n",
      "\n",
      "   Predicted  \n",
      "0  204165.53  \n",
      "1  353007.22  \n",
      "2  147208.89  \n",
      "3  198120.66  \n",
      "4  229650.58  \n",
      "\n",
      "Future Predictions:\n",
      "   Year_Recorded  County_Integer  State_Integer  AnnualValue_AllHomes  \\\n",
      "0           2024               1              1             202480.83   \n",
      "1           2024               2              1             349517.88   \n",
      "2           2024               3              1             145776.00   \n",
      "3           2024               4              1             198461.25   \n",
      "4           2024               5              1             229328.75   \n",
      "\n",
      "   Year_Recorded  Predicted  \n",
      "0           2025  204165.53  \n",
      "1           2025  353007.22  \n",
      "2           2025  147208.89  \n",
      "3           2025  198120.66  \n",
      "4           2025  229650.58  \n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the train, validation, and test sets\n",
    "train_predictions = xgb_model.predict(X_train_filtered)\n",
    "val_predictions = xgb_model.predict(X_val.drop(columns=['Year_Recorded']))\n",
    "test_predictions = xgb_model.predict(X_test_filtered)\n",
    "\n",
    "train_predictions_df = pd.DataFrame(train_predictions, columns=['Predicted'])\n",
    "train_years_df = pd.DataFrame(train_years, columns=['Year_Recorded','County_Integer','State_Integer','AnnualValue_AllHomes'])\n",
    "# Concatenate along columns (axis=1) or rows (axis=0)\n",
    "train_results = pd.concat([train_years_df, train_predictions_df], axis=1)\n",
    "\n",
    "val_predictions_df = pd.DataFrame(val_predictions, columns=['Predicted'])\n",
    "val_years_df = pd.DataFrame(val_years, columns=['Year_Recorded','County_Integer','State_Integer','AnnualValue_AllHomes'])\n",
    "# Concatenate along columns (axis=1) or rows (axis=0)\n",
    "val_results = pd.concat([val_years_df, val_predictions_df], axis=1)\n",
    "\n",
    "test_predictions_df = pd.DataFrame(test_predictions, columns=['Predicted'])\n",
    "test_years_df = pd.DataFrame(test_years, columns=['Year_Recorded','County_Integer','State_Integer','AnnualValue_AllHomes'])\n",
    "# Concatenate along columns (axis=1) or rows (axis=0)\n",
    "test_results = pd.concat([test_years_df, test_predictions_df], axis=1)\n",
    "\n",
    "future_predictions_results = pd.DataFrame({\n",
    "    'Year_Recorded': 2025,  \n",
    "    'Predicted': xgb_predictions\n",
    "})\n",
    "future_years_df = pd.DataFrame(test_years, columns=['County_Integer','State_Integer'])\n",
    "xgb_predictions_results = pd.concat([test_years_df, future_predictions_results], axis=1)\n",
    "\n",
    "# Display the results\n",
    "print(\"Train Results:\")\n",
    "print(train_results.head())\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(val_results.head())\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(test_results.head())\n",
    "\n",
    "print(\"\\nFuture Predictions:\")\n",
    "print(xgb_predictions_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation of models reveals \n",
    "# XGBoost emerged as the best-performing model, achieving an RMSE of 1566.90, MAE of 1155.61, \n",
    "# and an impressive R² of 1.00. This suggests that XGBoost was highly effective in capturing patterns within the data,\n",
    "# making it the most accurate model for this task.\n",
    "\n",
    "# Gradient Boosting followed closely, with an RMSE of 3036.49, MAE of 1898.68, and an R² of 1.00. \n",
    "# It demonstrated strong performance, showing its ability to handle complex relationships within the data, \n",
    "# contributing to its high accuracy.\n",
    "\n",
    "# Random Forest, though slightly less accurate than XGBoost, still performed exceptionally well, with an\n",
    "# RMSE of 44015.10, MAE of 34639.36, and R² of 0.54. It showed the ability to handle the data’s complexity \n",
    "# effectively and delivered reasonable results.\n",
    "\n",
    "# SARIMA, with an RMSE of 281064.66, MAE of 253197.74, and an R² of -17.77, underperformed significantly. \n",
    "# This indicates that SARIMA was not as effective at capturing the underlying patterns compared to tree-based models, \n",
    "# despite being a traditional time series method.\n",
    "\n",
    "# ARIMA also underperformed with an RMSE of 79606.35, MAE of 69014.93, and an R² of -0.51. \n",
    "# This highlights that traditional time series models struggled to match the accuracy of the ensemble models, \n",
    "# such as XGBoost and Gradient Boosting.\n",
    "\n",
    "# Prophet, with a notably high RMSE of 3.67 × 10¹⁸, MAE of 3.35 × 10¹⁸, and an extremely negative R² value, \n",
    "# showed the worst performance among the models. The results indicate that Prophet failed to capture the trends \n",
    "# correctly and made large errors, leading to poor predictions.\n",
    "\n",
    "# LSTM also underperformed, with an RMSE of 97844.89, MAE of 77337.87, and an R² of -1.27,\n",
    "# showing that it did not provide satisfactory results for this dataset.\n",
    "\n",
    "# Ridge Regression, with an RMSE of 8086.29, MAE of 7119.27, and R² of 0.98, performed reasonably well, \n",
    "# but it was not as effective as the ensemble methods like XGBoost, Gradient Boosting, and Random Forest.\n",
    "\n",
    "# Conclusion:\n",
    "# Based on these results, XGBoost is the best-performing model due to its low RMSE, high accuracy, \n",
    "# and ability to effectively capture complex patterns in the data. Ensemble models like XGBoost, Gradient Boosting,\n",
    "# and Random Forest proved to be the most reliable choices for this task, outperforming traditional models \n",
    "# such as SARIMA, ARIMA, Prophet, and LSTM. These ensemble methods showed the ability to handle complex relationships\n",
    "# within the data more effectively than the other approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
